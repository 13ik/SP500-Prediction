# %% [markdown]
# # S&P 500 Index Movement Prediction
# ## Financial Market Analysis and Forecasting System

# %% [markdown]
# ### 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import joblib
%matplotlib inline

# %% [markdown]
# ### 2. Load Dataset
# %%
try:
    df = pd.read_csv('../data/sp500_stocks.csv', parse_dates=['Date'])
    df = df.sort_values('Date').drop(columns=['Unnamed: 0'], errors='ignore')
    print("‚úÖ Data loaded successfully")
    print(f"Dataset shape: {df.shape}")
    print(f"Date range: {df['Date'].min().date()} to {df['Date'].max().date()}")
except Exception as e:
    print(f"‚ùå Error loading data: {str(e)}")

# %% [markdown]
# ### 3. Exploratory Data Analysis (EDA)
# %% [markdown]
# #### 3.1 Class Distribution
# %%
plt.figure(figsize=(10,6))
ax = df['sp500_increase'].value_counts().sort_index().plot.bar(
    color=['#e74c3c', '#f1c40f', '#2ecc71'],
    edgecolor='black'
)
plt.title('S&P 500 Movement Class Distribution', fontsize=14)
plt.xlabel('Movement Class (-1=Decline, 0=Stable, 1=Growth)', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=0)
ax.bar_label(ax.containers[0], label_type='edge')
plt.savefig('../images/class_distribution.png', bbox_inches='tight')
plt.show()

# %% [markdown]
# #### 3.2 Correlation Analysis
# %%
plt.figure(figsize=(14,10))
corr_matrix = df.corr(numeric_only=True)
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', fmt=".2f", 
           linewidths=0.5, annot_kws={"size": 10})
plt.title('Feature Correlation Matrix', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.savefig('../images/correlation_matrix.png', bbox_inches='tight')
plt.show()

# %% [markdown]
# ### 4. Data Preprocessing
# %%
features = [
    'ADBE_increase', 'INTC_increase', 'MSFT_increase',
    'AMD_increase', 'NVDA_increase', 'DIS_increase',
    'NFLX_increase', 'TSLA_increase', 'META_increase',
    'KFC_increase'
]

X = df[features]
y = df['sp500_increase']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# %% [markdown]
# ### 5. Model Training
# %% [markdown]
# #### 5.1 Train-Test Split
# %%
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# %% [markdown]
# #### 5.2 Hyperparameter Tuning
# %%
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(class_weight='balanced', random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# %% [markdown]
# #### 5.3 Best Model Selection
# %%
best_model = grid_search.best_estimator_
print("üî• Best Model Parameters:")
print(grid_search.best_params_)

# %% [markdown]
# ### 6. Model Evaluation
# %% [markdown]
# #### 6.1 Classification Report
# %%
y_pred = best_model.predict(X_test)
print("üìä Classification Performance:")
print(classification_report(y_test, y_pred, target_names=['Decline', 'Stable', 'Growth']))

# %% [markdown]
# #### 6.2 Confusion Matrix
# %%
plt.figure(figsize=(8,6))
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(cm, display_labels=['Decline', 'Stable', 'Growth'])
disp.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix', fontsize=14)
plt.savefig('../images/confusion_matrix.png', bbox_inches='tight')
plt.show()

# %% [markdown]
# #### 6.3 Feature Importance
# %%
plt.figure(figsize=(12,8))
feature_importance = pd.Series(best_model.feature_importances_, index=features)
feature_importance.nlargest(10).sort_values().plot(kind='barh', color='#3498db')
plt.title('Top 10 Important Features', fontsize=14)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.grid(axis='x', alpha=0.3)
plt.savefig('../images/feature_importance.png', bbox_inches='tight')
plt.show()

# %% [markdown]
# ### 7. Model Deployment
# %%
joblib.dump(best_model, '../models/sp500_model.pkl')
joblib.dump(scaler, '../models/scaler.pkl')
print("üíæ Model artifacts saved successfully")

# %% [markdown]
# ### 8. Unit Tests
# %%
def test_model_loading():
    try:
        model = joblib.load('../models/sp500_model.pkl')
        scaler = joblib.load('../models/scaler.pkl')
        assert model is not None and scaler is not None
        print("‚úÖ Model loading test passed")
    except Exception as e:
        print(f"‚ùå Model loading test failed: {str(e)}")

def test_prediction():
    sample_data = {feature: 0.0 for feature in features}
    sample_data['NVDA_increase'] = 0.05
    try:
        model = joblib.load('../models/sp500_model.pkl')
        scaler = joblib.load('../models/scaler.pkl')
        scaled_data = scaler.transform(pd.DataFrame([sample_data]))
        prediction = model.predict(scaled_data)
        assert prediction[0] in [-1, 0, 1]
        print("‚úÖ Prediction test passed")
    except Exception as e:
        print(f"‚ùå Prediction test failed: {str(e)}")

# Run tests
test_model_loading()
test_prediction()
